---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
  )
```

# evalITR

<!-- badges: start -->
[![R-CMD-check](https://github.com/MichaelLLi/evalITR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/MichaelLLi/evalITR/actions/workflows/R-CMD-check.yaml)
[![R-CMD-check](https://github.com/xiaolong-y/evalITR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/xiaolong-y/evalITR/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

## Installation

You can install the development version of evalITR from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("MichaelLLi/evalITR")
```


(Optional) if you have multiple cores, we recommendate using multisession futures and processing in parallel. This would increase computation efficiency and reduce the time to fit the model. 

```r
library(furrr)
library(future.apply)

nworkers <- 4
plan(multisession, workers =nworkers)
```


## Example under cross-validation

This is an example using the `star` dataset (for more information about the dataset, please use `?star`). 

We first load the dataset and specify both outcome variables (reading, math, and writing scores) and
covariates we want to include in the model. Then we use a series of machine learning
algorithms to estimate the heterogeneous effects of small classes on
educational attainment. We use 20% as a budget constraint and tuned the
model through through the 3-fold cross validation.

```{r message = FALSE}
library(tidyverse)
library(evalITR)

load("data/star.rda")

# specifying outcomes
outcomes <- c("g3tlangss",
                "g3treadss","g3tmathss")


# specifying covariates
covariates <-  star %>% 
                dplyr::select(-c(all_of(outcomes),"treatment")) %>% 
                colnames()

# estimate ITR 
set.seed(2021)
fit <- estimate_itr(outcome = outcomes,
               treatment = treatment,
               covariates = covariates,
               data = star_data,
                algorithms = c( "causal_forest", 
                  "bartc",
                  "bart",
                  "svm",
                  "lasso",
                  "boost", 
                  "random_forest",
                  "cart",),
               budget = 0.2,
               ratio = 0.7,
               n_folds = 7)


# evaluate ITR 
est <- evaluate_itr(fit)
```

Alternatively, we can train the model with the `caret` package (for further information about `caret`, see [caret](http://topepo.github.io/caret/index.html)). 

```{r caret estimate, message = FALSE}
# alternatively (with caret package)
fit_caret <- estimate_itr(outcome = outcomes,
               treatment = treatment,
               covariates = covariates,
               data = star_data,
               algorithms = c("caret"),
               budget = 0.2,
               ratio = 0.7,
               trainControl_method = "repeatedcv", # resampling method
               train_method = "gbm", # model
               preProcess = NULL, # pre-processing predictors
               tuneGrid = NULL, # tuning values
               tuneLength = 1, # granularity in tuning
               number = 3, # number of folds/resampling iterations
               repeats = 3)


est_caret <- evaluate_itr(fit_caret)
```

The`summary()` function displays the following summary statistics: (1) population average prescriptive effect `PAPE`; (2) population average prescriptive effect with a budget constraint `PAPEp`; (3) population average prescriptive effect difference with a budget constraint `PAPDp`. This quantity will be computed with more than 2 machine learning algorithms); (4) and area under the prescriptive effect curve `AUPEC`. For more information about these evaluation metrics, please refer to [Imai and Li (2021)](https://arxiv.org/abs/1905.05389); (5) Grouped Average Treatment Effects `GATEs`.  The details of the methods for this design are given in [Imai and Li (2022)](https://arxiv.org/abs/2203.14511).


```{r sp_summary}
# summarize estimates
summary(est)

# summarize estimates with model trained using caret
summary(est_caret)
```


We plot the estimated Area Under the Prescriptive Effect Curve for the writing score across a range of budget constraints for causal forest.

```{r sp_plot, fig.width=4, fig.height=3,fig.align = "center"}
# plot the AUPEC 
plot(est)

# plot the AUPEC for the model trained using caret
plot(est_caret)
```

## Example under cross-validation

The package also allows estimate ITR with k-folds cross-validation. Instead of specifying the `ratio` argument, we choose the number of folds (`n_folds`). The following code presents an example of estimating ITR with 3 folds cross-validation. In practice, we recommend using 10 folds to get a more stable model performance. 

```{r cv_estimate, message = FALSE}
# estimate ITR 
set.seed(2021)
fit_cv <- estimate_itr(outcome = outcomes,
               treatment = treatment,
               covariates = covariates,
               data = star,
               algorithms = c( "causal_forest", 
                  "bartc",
                  # "bart",
                  "svm",
                  "lasso",
                  "boost", 
                  "random_forest",
                  "cart",
                  "bagging"),
               budget = 0.2,
               n_folds = 4)

# evaluate ITR 
est_cv <- evaluate_itr(fit_cv)

```

We present the results with 3-folds cross validation and plot the AUPEC. 

```{r cv_summary}
# summarize estimates
summary(est_cv)
```


```{r sv_plot, fig.width=4, fig.height=3,fig.align = "center"}
# plot the AUPEC 
plot(est_cv)
```


## Example with multiple ML algorithms/outcomes

We can estimate ITR with various machine learning algorithms and then compare the performance of each model. The package includes 8 different ML algorithms (causal forest, BART, lasso, boosting trees, random forest, CART, bagging trees, svm).

The package also allows estimate heterogeneous treatment effects on the individual and group-level. On the individual-level, the summary statistics and the AUPEC plot show whether assigning individualized treatment rules may outperform complete random experiment. On the group-level, we specify the number of groups through `ngates` and estimating heterogeneous treatment effects across groups. 

If the original experiment has diverse outcome measures, we develop ITRs for each outcome and use them to estimate the heterogeneous effects across the different outcomes.


```{r multiple, message = FALSE}
# specifying outcomes
outcomes <- c("g3tlangss","g3treadss","g3tmathss")

# specifying covariates
covariates <-  star %>% dplyr::select(-c("g3tlangss","g3treadss","g3tmathss","treatment")) %>% colnames()

# train the model
set.seed(2021)
fit_cv <- estimate_itr(outcome = outcomes,

               treatment = "treatment",
               covariates = covariates,
               data = star,
               algorithms = c(
                  "causal_forest", 
                  "bartc",
                  # "bart",
                  "svm",
                  "lasso",
                  "boost", 
                  "random_forest",
                  "cart",
                  "bagging"),
               plim = 0.2,
               n_folds = 3,
               ratio = 0.67)
```

The`summary()` function displays the following summary statistics: (1) population average prescriptive effect `PAPE`; (2) population average prescriptive effect with a budget constraint `PAPEp`; (3) population average prescriptive effect difference with a budget constraint `PAPDp`; (4) and area under the prescriptive effect curve `AUPEC`. For more information about these evaluation metrics, please refer to [this paper](https://arxiv.org/abs/1905.05389).



```{r output}
# summarize estimates
summary(fit)
```

We plot the estimated Area Under the Prescriptive Effect Curve
for the writing score across a range of budget constraints for different algorithms.

```{r plot, fig.width=8, fig.height=8,fig.align = "center"}
# plot the AUPEC with different ML algorithms
plot(x = fit, 
      outcome = "g3tlangss",
      treatment = "treatment",
      data = star, 
      algorithms = c(
          "causal_forest",
          # "bart",
          "lasso",
          "boost", 
          "random_forest",
          "bagging", 
          "cart"))

```


## Example under sample splitting (under development)

Please set argument input of `n_folds` to 0 ion order to train the models under sample splitting. The split ratio between train and test set is determined by the `ratio` argument. 
```{r message = FALSE}
library(tidyverse)
library(evalITR)

load("data/star.rda")

# specifying outcomes
outcomes <- c("g3tlangss",
              "g3treadss",
              "g3tmathss")


# specifying covariates
covariates <-  star %>% 
                dplyr::select(-c(all_of(outcomes),"treatment")) %>% 
                colnames()

# estimate ITR 
fit <- run_itr(outcome = outcomes,
               treatment = "treatment",
               covariates = covariates,
               data = star,
               algorithms = c(
                  "causal_forest", 
                  "lasso",
                  "boost", 
                  "random_forest",
                  "bagging",
                  "cart"),
               plim = 0.2,
               n_folds = 0,
               ratio = 0.67)

glimpse(fit$qoi)
```

For group level average treatment effects (GATEs), we conduct hypothesis tests to examine whether the groups are 1) rank consistent; and 2) heterogeneous.

```{r multiple, message = FALSE}
# one outcome 
test <- test_itr(fit)
testcv <- test_itr(fit_cv)
testcaret <- test_itr(fit_caret)

T_t <- fit_cv[["estimates"]][[1]][["Tcv"]]
tau_t <- (gettaucv(fit_cv))
Y_t <- fit_cv[["estimates"]][[1]][["Ycv"]]
ind_t <- fit_cv[["estimates"]][[1]][["indcv"]]
ngates_t <- fit_cv[["estimates"]][[1]][["params"]][["ngates"]]

consistcv.test(T_t,
               tau_t,
               Y_t,
               ind_t,
               ngates_t)

hetcv.test(T_t,
               tau_t,
               Y_t,
               ind_t,
               ngates_t)

 T = c(1,0,1,0,1,0,1,0)
 tau = matrix(c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,-0.5,-0.3,-0.1,0.1,0.3,0.5,0.7,0.9),nrow = 8, ncol = 2)
 Y = c(4,5,0,2,4,1,-4,3)
 ind = c(rep(1,4),rep(2,4))
 consisttestlist <- consistcv.test(T,tau,Y,ind,ngates=2)
 consisttestlist$stat
 consisttestlist$pval
 
 # all same length; dim(tau)[2] = ngates
 View(T)
 View(tau)
 View(ind)
 View(tau)
 
 
 length(fit_cv[["estimates"]][[1]][["Ycv"]])
 dim(fit_cv[["estimates"]][[1]][["fit_ml"]][["causal_forest"]][[1]][["tau"]])[1]
 
#' Conduct hypothesis tests
#' @param fit Fitted model. Usually an output from \code{estimate_itr}
#' @param ngates The number of groups to separate the data into. The groups are determined by \code{tau}. Default is 5.
#' @param nsim Number of Monte Carlo simulations used to simulate the null distributions. Default is 10000.
#' @param ... Further arguments passed to the function.
#' @return An object of \code{itr} class
#' @export
test_itr <- function(
    fit,
    ngates = 5,
    nsim = 10000,
    ...
) {
  
  # test parameters
  estimates  <- fit$estimates
  cv         <- estimates[[1]]$params$cv
  fit_ml     <- estimates[[1]]$fit_ml
  Tcv        <- estimates[[1]]$Tcv
  Ycv        <- estimates[[1]]$Ycv
  indcv      <- estimates[[1]]$indcv
  algorithms <- fit$df$algorithms
  outcome    <- fit$df$outcome
  
  # run tests
  
  ## =================================
  ## sample splitting
  ## =================================
  
  if(cv == FALSE){
  cat('Conduct hypothesis tests for GATEs unde sample splitting ...\n')
  
  # loop over all outcomes
  for (m in 1:length(outcome)) {

    ## run consistency tests for each model 
    if ("causal_forest" %in% algorithms) {
      consist[[m]] <- consist.test(
        T   = Tcv,
        tau = fit_ml$causal_forest$tau, 
        Y   = Ycv,
        ngates = ngates)  
    }

    ## run heterogeneity tests for each model 
    if ("causal_forest" %in% algorithms) {
      het[[m]] <- het.test(
        T   = Tcv,
        tau = fit_ml$causal_forest$tau, 
        Y   = Ycv,
        ngates = ngates)  
    }
    
  }
}

  ## =================================
  ## cross validation
  ## =================================
    
  if(cv == TRUE){
  cat('Conduct hypothesis tests for GATEs unde cross-validation ...\n')
    
  # loop over all outcomes
  for (m in 1:length(outcome)) {

    ## run consistency tests for each model 
    if ("causal_forest" %in% algorithms) {
      consistcv[[m]] <- consistcv.test(
        T   = Tcv,
        tau = fit_ml$causal_forest$tau, 
        Y   = Ycv,
        ngates = ngates)  
    }

    ## run heterogeneity tests for each model 
    if ("causal_forest" %in% algorithms) {
      hetcv[[m]] <- het.test(
        T   = Tcv,
        tau = fit_ml$causal_forest$tau, 
        Y   = Ycv,
        ind = indcv,
        ngates = ngates)  
    }
    

    #' consisttestlist <- consist.test(T,tau,Y,ngates=5)
    ## run heterogeneity tests for each model 
  }
  }

  tests <- list(
    consist = consist,
    het = het,
    consistcv = consistcv,
    hetcv = hetcv    
  )
  
  return(tests)
}



      if("svm" %in% algorithms){
        fit_ml[["svm"]][[j]] <- run_svm(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }


      if("bartc" %in% algorithms){
        fit_ml[["bartc"]][[j]] <- run_bartc(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("bart" %in% algorithms){
        fit_ml[["bart"]][[j]] <- run_bartmachine(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("boost" %in% algorithms){
        fit_ml[["boost"]][[j]] <- run_boost(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("random_forest" %in% algorithms){
        fit_ml[["random_forest"]][[j]] <- run_random_forest(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("bagging" %in% algorithms){
        fit_ml[["bagging"]][[j]] <- run_bagging(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("cart" %in% algorithms){
        fit_ml[["cart"]][[j]] <- run_cart(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
      }

      if("caret" %in% algorithms){
        fit_ml[["caret"]][[j]] <- run_caret(
          dat_train = training_data_elements,
          dat_test  = testing_data_elements,
          dat_total = total_data_elements,
          params    = params,
          indcv     = indcv,
          iter      = j,
          budget    = budget
        )
```


